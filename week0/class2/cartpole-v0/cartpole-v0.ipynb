{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 input nodes\n",
    "# ReLu\n",
    "# 16 hidden layer nodes\n",
    "# ReLu \n",
    "# 2 output nodes\n",
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:423: UserWarning: \u001b[33mWARN: Custom namespace `ALE` is being overridden by namespace `ALE`. If you are developing a plugin you shouldn't specify a namespace in `register` calls. The namespace is specified through the entry point package metadata.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import pickle\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nenv = gym.make(\"CartPole-v0\")\\nobs = env.reset()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "obs = env.reset()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\naction = env.action_space.sample()\\nobs, reward, done, _ = env.step(action)\\nenv.render()\\nprint(obs)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "action = env.action_space.sample()\n",
    "obs, reward, done, _ = env.step(action)\n",
    "env.render()\n",
    "print(obs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nenv.close()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "env.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of mutation\n",
    "- a = [1, 2, 3, 4, 5]\n",
    "- mut_var = .1\n",
    "- b = [1.1, 1.9, 3.1, 3.9, 4.9]\n",
    "\n",
    "- for i in range(len(a)):\n",
    "    - a[i] += random.choice([-1, 1]) * mut_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "POPULATION = 32\n",
    "GENERATIONS = 5\n",
    "\n",
    "EPISODES = 8\n",
    "MAX_EP_LEN = 100000\n",
    "\n",
    "MUTATION_VARIANCE = 0.1\n",
    "\n",
    "MODEL_PATH = \"model.bin\"\n",
    "\n",
    "ARCH = (4, 16, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a):\n",
    "    return np.where(a > 0, a, 0)\n",
    "\n",
    "def softmax(a):\n",
    "    # https://www.pythonpool.com/numpy-softmax/#:~:text=Numpy%20softmax%20is%20a%20mathematical%20function%20that%20takes,It%20normalizes%20an%20input%20to%20a%20probability%20distribution.\n",
    "    return np.exp(a) / np.sum(np.exp(a))\n",
    "\n",
    "def time_elapsed(prev, cur, round_digit=8):\n",
    "    delta_time = cur - prev\n",
    "    return f\"\\tTime elapsed: {round(delta_time, round_digit)} seconds\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, arch, net_to_copy=None, mut_var=MUTATION_VARIANCE):\n",
    "        \"\"\"\n",
    "        Architecture\n",
    "\n",
    "        4 Input nodes with 16 weights each to 16 hidden layer nodes\n",
    "        16 Hidden nodes with 2 weights each to 2 output layer nodes\n",
    "\n",
    "        16 Biases for each of the hidden layer nodes\n",
    "        2 Biases for each of the output layer nodes\n",
    "\n",
    "        (1, 16) -> 1 Layer of 16 Nodes\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if net_to_copy is None:\n",
    "            weights = [\n",
    "                np.random.rand(arch[1], arch[0]),\n",
    "                np.random.rand(arch[2], arch[1]),\n",
    "            ]\n",
    "\n",
    "            biases = [\n",
    "                np.zeros((arch[1],)),\n",
    "                np.zeros((arch[2],)),\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            weights = net_to_copy.weights\n",
    "            biases = net_to_copy.biases\n",
    "\n",
    "            weights = [\n",
    "                weight + np.random.normal(0.0, mut_var, weight.shape)\n",
    "                for weight in weights\n",
    "            ]\n",
    "            biases = [\n",
    "                bias + np.random.normal(0.0, mut_var, bias.shape) for bias in biases\n",
    "            ]\n",
    "            \n",
    "        # print([weight.shape for weight in weights])\n",
    "        # print([bias.shape for bias in biases])\n",
    "\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "    def forward(self, _obs):\n",
    "        weights = self.weights\n",
    "        biases = self.biases\n",
    "\n",
    "        _obs = np.array(_obs)\n",
    "\n",
    "        hidden = relu(np.dot(weights[0], _obs) + biases[0])\n",
    "        # print(hidden.shape)\n",
    "\n",
    "        output = relu(np.dot(weights[1], hidden) + biases[1])\n",
    "        # print(output.shape)\n",
    "\n",
    "        # print(output)\n",
    "\n",
    "        probabilities = softmax(output)\n",
    "        return np.argmax(\n",
    "            probabilities\n",
    "        )  # Returns index of highest element --> 0, 1 (Perfect for this action space)\n",
    "\n",
    "    def evaluate(\n",
    "        self, episodes=EPISODES, max_ep_len=MAX_EP_LEN, render_env=False, record=False\n",
    "    ):\n",
    "        ep_rewards = []\n",
    "        env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "        if record:\n",
    "            env = gym.wrappers.Monitor(env, \"recording\")\n",
    "\n",
    "        env._max_episode_steps = max_ep_len\n",
    "\n",
    "        for i_ep in range(episodes):\n",
    "            _obs = env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "\n",
    "            while not done:\n",
    "                if render_env:\n",
    "                    env.render()\n",
    "                    \n",
    "                action = self.forward(_obs)\n",
    "                _obs, reward, done, _ = env.step(action)\n",
    "                score += reward\n",
    "\n",
    "            ep_rewards.append(score)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        return np.array(ep_rewards).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = NN(ARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.forward([1, 2, 3, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.evaluate(render_env=True, record=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GA:\n",
    "    def __init__(\n",
    "        self,\n",
    "        arch=ARCH,\n",
    "        pop=POPULATION,\n",
    "        gens=GENERATIONS,\n",
    "        mut_var=MUTATION_VARIANCE,\n",
    "        verbose=False,\n",
    "        eps=EPISODES,\n",
    "        max_ep_len=MAX_EP_LEN,\n",
    "        render_env=False,\n",
    "        record=False,\n",
    "    ):\n",
    "        self.arch = arch\n",
    "        self.pop = pop\n",
    "        self.gens = gens\n",
    "        self.mut_var = mut_var\n",
    "        self.verbose = verbose\n",
    "        self.eps = eps\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.render_env = render_env\n",
    "        self.record = record\n",
    "\n",
    "        self.nets = [NN(arch) for _ in range(pop)]\n",
    "\n",
    "    def get_best_fit(self):\n",
    "        fitness_arr = []\n",
    "        for i, net in enumerate(self.nets):\n",
    "            prev_time = time()\n",
    "\n",
    "            fitness = self.evaluate(net)\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    \"Network:\",\n",
    "                    i + 1,\n",
    "                    \"\\tFitness:\",\n",
    "                    round(fitness),\n",
    "                    time_elapsed(prev_time, time()),\n",
    "                )\n",
    "\n",
    "            fitness_arr.append(fitness)\n",
    "\n",
    "            if fitness == self.max_ep_len:\n",
    "                break\n",
    "\n",
    "        fitnesses = np.array(fitness_arr)\n",
    "        i_best_net = np.argmax(fitness_arr)\n",
    "        best_net_score = fitnesses[i_best_net]\n",
    "        best_net = self.nets[i_best_net]\n",
    "\n",
    "        # best_net.evaluate(self.eps, self.max_ep_len, True)\n",
    "\n",
    "        return best_net, best_net_score, fitnesses\n",
    "\n",
    "    def mutate_nets(self, best_net):\n",
    "        new_nets = [NN(self.arch, best_net, self.mut_var) for _ in range(self.pop - 1)]\n",
    "        return new_nets\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.gens):\n",
    "            prev_time = time()\n",
    "\n",
    "            best_net, best_net_score, fitnesses = self.get_best_fit()\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    \"=====\",\n",
    "                    \"Generation:\",\n",
    "                    i + 1,\n",
    "                    \"\\tMaximum Reward:\",\n",
    "                    round(fitnesses.max()),\n",
    "                    \"\\tAverage Reward:\",\n",
    "                    round(fitnesses.mean()),\n",
    "                    time_elapsed(prev_time, time(), 3),\n",
    "                    \"=====\",\n",
    "                    \"\\n\",\n",
    "                )\n",
    "\n",
    "            # print(best_net_score)\n",
    "            if best_net_score == self.max_ep_len:\n",
    "                print(\"Callback reached: Max score obtained.\")\n",
    "                break\n",
    "\n",
    "            new_nets = self.mutate_nets(best_net)\n",
    "\n",
    "            self.nets = [best_net] + new_nets\n",
    "\n",
    "        self.best_net = best_net\n",
    "\n",
    "    def load(self, path=MODEL_PATH):\n",
    "        with open(path, \"rb\") as f:\n",
    "            self.best_net = pickle.load(f)\n",
    "\n",
    "    def save(self, path=MODEL_PATH):\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self.best_net, f)\n",
    "\n",
    "    def evaluate(self, net):\n",
    "        mean_rewards = net.evaluate(\n",
    "            self.eps, self.max_ep_len, self.render_env, self.record\n",
    "        )\n",
    "        return mean_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ga = GA(verbose=True)\n",
    "# model_path = MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n===== Generation: 2 \\tMaximum Reward: 10000 \\tAverage Reward: 2734 \\tTime elapsed: 10.692 seconds ===== \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ga = GA(verbose=True, arch=(4, 8, 2), max_ep_len=10000, pop=64)\n",
    "# model_path = \"model-8 nodes-10k eps.bin\"\n",
    "\n",
    "\"\"\"\n",
    "===== Generation: 2 \tMaximum Reward: 10000 \tAverage Reward: 2734 \tTime elapsed: 10.692 seconds ===== \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n===== Generation: 1 \\tMaximum Reward: 5000 \\tAverage Reward: 170 \\tTime elapsed: 0.814 seconds =====\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ga = GA(verbose=True, arch=(4, 4, 2), max_ep_len=5000, pop=64, eps=2)\n",
    "# model_path = \"model-4 nodes-5k eps.bin\"\n",
    "\n",
    "\"\"\"\n",
    "===== Generation: 1 \tMaximum Reward: 5000 \tAverage Reward: 170 \tTime elapsed: 0.814 seconds =====\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n===== Generation: 1 \\tMaximum Reward: 200 \\tAverage Reward: 104 \\tTime elapsed: 0.018 seconds =====\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ga = GA(verbose=True, arch=(4, 1, 2), max_ep_len=200, pop=64, eps=1)\n",
    "model_path = \"model-too fast.bin\"\n",
    "\n",
    "\"\"\"\n",
    "===== Generation: 1 \tMaximum Reward: 200 \tAverage Reward: 104 \tTime elapsed: 0.018 seconds =====\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save model\n",
    "# ga.train()\n",
    "# ga.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jason\\Desktop\\2022-05-23-PRG - RPI Learning and Advanced Game AI Summer 2022\\week0\\class2\\cartpole-v0\\cartpole-v0.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week0/class2/cartpole-v0/cartpole-v0.ipynb#ch0000018?line=5'>6</a>\u001b[0m     ga\u001b[39m.\u001b[39mbest_net\u001b[39m.\u001b[39mevaluate(\u001b[39m1\u001b[39m, \u001b[39m200\u001b[39m, \u001b[39mTrue\u001b[39;00m, \u001b[39mTrue\u001b[39;00m) \u001b[39m# Record only records 200 steps\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week0/class2/cartpole-v0/cartpole-v0.ipynb#ch0000018?line=6'>7</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week0/class2/cartpole-v0/cartpole-v0.ipynb#ch0000018?line=7'>8</a>\u001b[0m     ga\u001b[39m.\u001b[39;49mbest_net\u001b[39m.\u001b[39;49mevaluate(\u001b[39m1\u001b[39;49m, \u001b[39m1000000\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\jason\\Desktop\\2022-05-23-PRG - RPI Learning and Advanced Game AI Summer 2022\\week0\\class2\\cartpole-v0\\cartpole-v0.ipynb Cell 9'\u001b[0m in \u001b[0;36mNN.evaluate\u001b[1;34m(self, episodes, max_ep_len, render_env, record)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week0/class2/cartpole-v0/cartpole-v0.ipynb#ch0000008?line=79'>80</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week0/class2/cartpole-v0/cartpole-v0.ipynb#ch0000008?line=80'>81</a>\u001b[0m     \u001b[39mif\u001b[39;00m render_env:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week0/class2/cartpole-v0/cartpole-v0.ipynb#ch0000008?line=81'>82</a>\u001b[0m         env\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week0/class2/cartpole-v0/cartpole-v0.ipynb#ch0000008?line=83'>84</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(_obs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week0/class2/cartpole-v0/cartpole-v0.ipynb#ch0000008?line=84'>85</a>\u001b[0m     _obs, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gym\\core.py:328\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/core.py?line=325'>326</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/core.py?line=326'>327</a>\u001b[0m     \u001b[39m\"\"\"Renders the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/core.py?line=327'>328</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=45'>46</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=46'>47</a>\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=47'>48</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=48'>49</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=49'>50</a>\u001b[0m     )\n\u001b[1;32m---> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=50'>51</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:273\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/cartpole.py?line=270'>271</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/cartpole.py?line=271'>272</a>\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m--> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/cartpole.py?line=272'>273</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/cartpole.py?line=273'>274</a>\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/cartpole.py?line=275'>276</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ga.load(model_path)\n",
    "\n",
    "record = False\n",
    "\n",
    "if record:\n",
    "    ga.best_net.evaluate(1, 200, True, True) # Record only records 200 steps\n",
    "else:\n",
    "    ga.best_net.evaluate(1, 1000000, True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7c1fa83a45090c7dcf13129008a160cc437ddcdb7cd57cdda701723205ddb7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
