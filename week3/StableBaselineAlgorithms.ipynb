{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm:\n",
    "    def __init__(self, env_name):\n",
    "        env_path = os.path.join(\"Training\", env_name)\n",
    "        log_path = os.path.join(env_path, \"Logs\")\n",
    "        self.PPO_path = os.path.join(env_path, \"Saved Models\", \"PPO\")\n",
    "        self.DQN_path = os.path.join(env_path, \"Saved Models\", \"DQN\")\n",
    "        self.A2C_path = os.path.join(env_path, \"Saved Models\", \"A2C\")\n",
    "\n",
    "        self.env = gym.make(env_name)\n",
    "        self.PPO_model = PPO(\"MlpPolicy\", self.env, verbose=1, tensorboard_log=log_path)\n",
    "        self.DQN_model = DQN(\"MlpPolicy\", self.env, verbose=1, tensorboard_log=log_path)\n",
    "        self.A2C_model = A2C(\"MlpPolicy\", self.env, verbose=1, tensorboard_log=log_path)\n",
    "\n",
    "    def learn(self, timesteps=100000, save=True):\n",
    "        self.PPO_model.learn(timesteps)\n",
    "        self.DQN_model.learn(timesteps)\n",
    "        self.A2C_model.learn(timesteps)\n",
    "        \n",
    "        if save:\n",
    "            self.save()\n",
    "\n",
    "    def save(self):\n",
    "        self.PPO_model.save(self.PPO_path)\n",
    "        self.DQN_model.save(self.DQN_path)\n",
    "        self.A2C_model.save(self.A2C_path)\n",
    "\n",
    "    def load(self):\n",
    "        self.PPO_model = PPO.load(self.PPO_path, env=self.env)\n",
    "        self.DQN_model = DQN.load(self.DQN_path, env=self.env)\n",
    "        self.A2C_model = A2C.load(self.A2C_path, env=self.env)\n",
    "\n",
    "    def evaluate(self):\n",
    "        evaluate_policy(self.PPO_model, self.env, n_eval_episodes=1, render=True)\n",
    "        evaluate_policy(self.DQN_model, self.env, n_eval_episodes=1, render=True)\n",
    "        evaluate_policy(self.A2C_model, self.env, n_eval_episodes=1, render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "ab = Algorithm(\"Acrobot-v1\")\n",
    "cp = Algorithm(\"CartPole-v0\")\n",
    "mc = Algorithm(\"MountainCar-v0\")\n",
    "\n",
    "tx = Algorithm(\"Taxi-v3\")\n",
    "cw = Algorithm(\"CliffWalking-v0\")\n",
    "fl = Algorithm(\"FrozenLake-v1\")\n",
    "\n",
    "ll = Algorithm(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.learn()\n",
    "cp.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.learn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx.learn()\n",
    "cw.learn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\FrozenLake-v1\\Logs\\PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.74     |\n",
      "|    ep_rew_mean     | 0.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 189      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.09        |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010957291 |\n",
      "|    clip_fraction        | 0.0348      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.807      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0249      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0072     |\n",
      "|    value_loss           | 0.0174      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.53        |\n",
      "|    ep_rew_mean          | 0.04        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012648432 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.0938      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.03       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.0185      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.2         |\n",
      "|    ep_rew_mean          | 0.02        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009364937 |\n",
      "|    clip_fraction        | 0.0638      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0202      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00871    |\n",
      "|    value_loss           | 0.0196      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.98        |\n",
      "|    ep_rew_mean          | 0.02        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012449525 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00392     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 0.0163      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.22        |\n",
      "|    ep_rew_mean          | 0.03        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011733915 |\n",
      "|    clip_fraction        | 0.0835      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0211     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 0.0172      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.21        |\n",
      "|    ep_rew_mean          | 0.04        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011085688 |\n",
      "|    clip_fraction        | 0.0999      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00392    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 0.0277      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.91        |\n",
      "|    ep_rew_mean          | 0.06        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 127         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011229423 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0158      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    value_loss           | 0.024       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.4        |\n",
      "|    ep_rew_mean          | 0.07        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010512933 |\n",
      "|    clip_fraction        | 0.0979      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00152    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.009      |\n",
      "|    value_loss           | 0.0372      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.3        |\n",
      "|    ep_rew_mean          | 0.08        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015736124 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0197      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00972    |\n",
      "|    value_loss           | 0.0401      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.4        |\n",
      "|    ep_rew_mean          | 0.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 173         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010380749 |\n",
      "|    clip_fraction        | 0.0957      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.0942      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0262     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00635    |\n",
      "|    value_loss           | 0.0435      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.6        |\n",
      "|    ep_rew_mean          | 0.08        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 188         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009466509 |\n",
      "|    clip_fraction        | 0.0709      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0191      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00817    |\n",
      "|    value_loss           | 0.0583      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.1        |\n",
      "|    ep_rew_mean          | 0.1         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 204         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006876461 |\n",
      "|    clip_fraction        | 0.0921      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00347     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00673    |\n",
      "|    value_loss           | 0.0424      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.4        |\n",
      "|    ep_rew_mean          | 0.13        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 221         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009932115 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.973      |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0159      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 0.046       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.1        |\n",
      "|    ep_rew_mean          | 0.19        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 240         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008018804 |\n",
      "|    clip_fraction        | 0.0663      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0293      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00534    |\n",
      "|    value_loss           | 0.0457      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14          |\n",
      "|    ep_rew_mean          | 0.12        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 267         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009175347 |\n",
      "|    clip_fraction        | 0.073       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.874      |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0339      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00755    |\n",
      "|    value_loss           | 0.061       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.2        |\n",
      "|    ep_rew_mean          | 0.18        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 294         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007882483 |\n",
      "|    clip_fraction        | 0.0781      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.826      |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0323      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0063     |\n",
      "|    value_loss           | 0.0556      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.7        |\n",
      "|    ep_rew_mean          | 0.19        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 324         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008415097 |\n",
      "|    clip_fraction        | 0.0668      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.796      |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0131      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    value_loss           | 0.0615      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 18.6         |\n",
      "|    ep_rew_mean          | 0.16         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 110          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 352          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061633512 |\n",
      "|    clip_fraction        | 0.0533       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.765       |\n",
      "|    explained_variance   | 0.186        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0161       |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00466     |\n",
      "|    value_loss           | 0.0726       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 20.9         |\n",
      "|    ep_rew_mean          | 0.24         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 107          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 381          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050993655 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.735       |\n",
      "|    explained_variance   | 0.0961       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00865      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00424     |\n",
      "|    value_loss           | 0.054        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20.1        |\n",
      "|    ep_rew_mean          | 0.24        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 404         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009373707 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.745      |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00488     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00979    |\n",
      "|    value_loss           | 0.0623      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.6        |\n",
      "|    ep_rew_mean          | 0.2         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 417         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005434445 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.745      |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0281      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00381    |\n",
      "|    value_loss           | 0.0628      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.9        |\n",
      "|    ep_rew_mean          | 0.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 430         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008824326 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.664      |\n",
      "|    explained_variance   | 0.07        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.015       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 0.0565      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.4        |\n",
      "|    ep_rew_mean          | 0.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 442         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004611385 |\n",
      "|    clip_fraction        | 0.0441      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.653      |\n",
      "|    explained_variance   | 0.0938      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0256      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00435    |\n",
      "|    value_loss           | 0.0596      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 29.3         |\n",
      "|    ep_rew_mean          | 0.25         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 112          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 455          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062469407 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.627       |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.025        |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00499     |\n",
      "|    value_loss           | 0.0629       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 28.9        |\n",
      "|    ep_rew_mean          | 0.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 469         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008544406 |\n",
      "|    clip_fraction        | 0.0797      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.614      |\n",
      "|    explained_variance   | 0.0971      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0158      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00566    |\n",
      "|    value_loss           | 0.042       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 28          |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 484         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007827252 |\n",
      "|    clip_fraction        | 0.0434      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0109      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0085     |\n",
      "|    value_loss           | 0.0548      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 29.5         |\n",
      "|    ep_rew_mean          | 0.42         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 115          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 497          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043004104 |\n",
      "|    clip_fraction        | 0.0435       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.549       |\n",
      "|    explained_variance   | 0.244        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.018        |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.0044      |\n",
      "|    value_loss           | 0.0647       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 33.6         |\n",
      "|    ep_rew_mean          | 0.42         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 116          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 509          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054426845 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.543       |\n",
      "|    explained_variance   | 0.14         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.018        |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00513     |\n",
      "|    value_loss           | 0.0546       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 28          |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 520         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005219928 |\n",
      "|    clip_fraction        | 0.0545      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.471      |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0415      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00431    |\n",
      "|    value_loss           | 0.0528      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30.1         |\n",
      "|    ep_rew_mean          | 0.47         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 119          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 532          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031095245 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.449       |\n",
      "|    explained_variance   | 0.227        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.027        |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00628     |\n",
      "|    value_loss           | 0.0629       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 33.5         |\n",
      "|    ep_rew_mean          | 0.49         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 544          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022866288 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.452       |\n",
      "|    explained_variance   | 0.181        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.027        |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00369     |\n",
      "|    value_loss           | 0.0584       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 34.2         |\n",
      "|    ep_rew_mean          | 0.59         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 121          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 555          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028412016 |\n",
      "|    clip_fraction        | 0.0318       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.438       |\n",
      "|    explained_variance   | 0.3          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0168       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00305     |\n",
      "|    value_loss           | 0.0493       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jason\\Desktop\\PRG-2022-05-23 - RPI Learning and Advanced Game AI Summer 2022\\week3\\StableBaselineAlgorithms.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/PRG-2022-05-23%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week3/StableBaselineAlgorithms.ipynb#ch0000006?line=0'>1</a>\u001b[0m fl\u001b[39m.\u001b[39;49mlearn()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/PRG-2022-05-23%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week3/StableBaselineAlgorithms.ipynb#ch0000006?line=1'>2</a>\u001b[0m ll\u001b[39m.\u001b[39mlearn()\n",
      "\u001b[1;32mc:\\Users\\jason\\Desktop\\PRG-2022-05-23 - RPI Learning and Advanced Game AI Summer 2022\\week3\\StableBaselineAlgorithms.ipynb Cell 2'\u001b[0m in \u001b[0;36mAlgorithm.learn\u001b[1;34m(self, timesteps, save)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/PRG-2022-05-23%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week3/StableBaselineAlgorithms.ipynb#ch0000001?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\u001b[39mself\u001b[39m, timesteps\u001b[39m=\u001b[39m\u001b[39m100000\u001b[39m, save\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/PRG-2022-05-23%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week3/StableBaselineAlgorithms.ipynb#ch0000001?line=14'>15</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPPO_model\u001b[39m.\u001b[39;49mlearn(timesteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/PRG-2022-05-23%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week3/StableBaselineAlgorithms.ipynb#ch0000001?line=15'>16</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDQN_model\u001b[39m.\u001b[39mlearn(timesteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/PRG-2022-05-23%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week3/StableBaselineAlgorithms.ipynb#ch0000001?line=16'>17</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA2C_model\u001b[39m.\u001b[39mlearn(timesteps)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:304\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=290'>291</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=291'>292</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=292'>293</a>\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=300'>301</a>\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=301'>302</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=303'>304</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(PPO, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=304'>305</a>\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=305'>306</a>\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=306'>307</a>\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=307'>308</a>\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=308'>309</a>\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=309'>310</a>\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=310'>311</a>\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=311'>312</a>\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=312'>313</a>\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/ppo/ppo.py?line=313'>314</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:250\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=245'>246</a>\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=247'>248</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=249'>250</a>\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=251'>252</a>\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=252'>253</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:169\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=165'>166</a>\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=166'>167</a>\u001b[0m     \u001b[39m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=167'>168</a>\u001b[0m     obs_tensor \u001b[39m=\u001b[39m obs_as_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=168'>169</a>\u001b[0m     actions, values, log_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(obs_tensor)\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=169'>170</a>\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=171'>172</a>\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py:589\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/policies.py?line=586'>587</a>\u001b[0m \u001b[39m# Preprocess the observation if needed\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/policies.py?line=587'>588</a>\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_features(obs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/policies.py?line=588'>589</a>\u001b[0m latent_pi, latent_vf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_extractor(features)\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/policies.py?line=589'>590</a>\u001b[0m \u001b[39m# Evaluate the values for the given observations\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/policies.py?line=590'>591</a>\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_net(latent_vf)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:230\u001b[0m, in \u001b[0;36mMlpExtractor.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/torch_layers.py?line=224'>225</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/torch_layers.py?line=225'>226</a>\u001b[0m \u001b[39m:return: latent_policy, latent_value of the specified network.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/torch_layers.py?line=226'>227</a>\u001b[0m \u001b[39m    If all layers are shared, then ``latent_policy == latent_value``\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/torch_layers.py?line=227'>228</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/torch_layers.py?line=228'>229</a>\u001b[0m shared_latent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared_net(features)\n\u001b[1;32m--> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/stable_baselines3/common/torch_layers.py?line=229'>230</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_net(shared_latent), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_net(shared_latent)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ll.learn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cp.load()\n",
    "cp.evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.load()\n",
    "ab.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ll.load()\n",
    "ll.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=\"./\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7c1fa83a45090c7dcf13129008a160cc437ddcdb7cd57cdda701723205ddb7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
