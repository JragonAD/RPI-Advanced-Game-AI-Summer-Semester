{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Using Q-Learning\n",
    "\n",
    "[CartPole Documentation](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n",
    "\n",
    "| Num | Action                 |\n",
    "|-----|------------------------|\n",
    "| 0   | Push cart to the left  |\n",
    "| 1   | Push cart to the right |\n",
    "\n",
    "**Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "\n",
    "| Num | Observation           | Min                 | Max               |\n",
    "|-----|-----------------------|---------------------|-------------------|\n",
    "| 0   | Cart Position         | -4.8                | 4.8               |\n",
    "| 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "| 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "\n",
    "**Note:** While the ranges above denote the possible values for observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "\n",
    "- The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates if the cart leaves the `(-2.4, 2.4)` range.\n",
    "- The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Table\n",
    "\n",
    "| Num | Start | Step Width | Steps (Inc. 0) |\n",
    "|-----|-------|------------|----------------|\n",
    "| 0   | -2.4  | 0.1        | 48 + 1         |\n",
    "| 1   | -4    | 0.01       | 800 + 1        |\n",
    "| 2   | -12   | 1°         | 24 + 1         |\n",
    "| 3   | -4    | 0.01       | 800 + 1        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with all 4 observations spaces\n",
    "\n",
    "- Not actually necessary\n",
    "- Truly only need a q-table for all the angles the pole can be at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nenv = gym.make(\"CartPole-v1\")\\n\\nmin_vals = [0, 0, 0, 0]\\nmax_vals = [0, 0, 0, 0]\\n\\nepisodes = 100000\\nfor i_ep in range(episodes):\\n    # print(\"Episode:\", i_ep + 1)\\n\\n    obs = env.reset()\\n    done = False\\n\\n    while not done:\\n        # env.render()\\n\\n        action = env.action_space.sample()\\n\\n        obs, reward, done, _ = env.step(action)\\n        \\n        if done: \\n            break\\n        \\n        for i in range(len(obs)):\\n            min_vals[i] = min(obs[i], min_vals[i])\\n            max_vals[i] = max(obs[i], max_vals[i])\\n\\nprint(\"Min:\", min_vals)\\nprint(\"Max:\", max_vals)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Searching for ranges of observation space \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "min_vals = [0, 0, 0, 0]\n",
    "max_vals = [0, 0, 0, 0]\n",
    "\n",
    "episodes = 100000\n",
    "for i_ep in range(episodes):\n",
    "    # print(\"Episode:\", i_ep + 1)\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # env.render()\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done: \n",
    "            break\n",
    "        \n",
    "        for i in range(len(obs)):\n",
    "            min_vals[i] = min(obs[i], min_vals[i])\n",
    "            max_vals[i] = max(obs[i], max_vals[i])\n",
    "\n",
    "print(\"Min:\", min_vals)\n",
    "print(\"Max:\", max_vals)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nq_table_params = {\\n    \"start\": [-2.4, -4, -12, -4],\\n    \"step\": [1, 2, 0, 2], # In digits after the decimal point\\n    \"steps\": [49, 801, 25, 801],\\n}\\n\\nq_table = np.array(\\n    [np.zeros((steps_val,)) for steps_val in q_table_params[\"steps\"]], dtype=\"object\"\\n)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "q_table_params = {\n",
    "    \"start\": [-2.4, -4, -12, -4],\n",
    "    \"step\": [1, 2, 0, 2], # In digits after the decimal point\n",
    "    \"steps\": [49, 801, 25, 801],\n",
    "}\n",
    "\n",
    "q_table = np.array(\n",
    "    [np.zeros((steps_val,)) for steps_val in q_table_params[\"steps\"]], dtype=\"object\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nenv = gym.make(\"CartPole-v1\")\\n\\nepisodes = 10000\\nfor i_ep in range(episodes):\\n    obs = env.reset()\\n    done = False\\n\\n    while not done:\\n        action = env.action_space.sample()\\n\\n        obs, reward, done, _ = env.step(action)\\n        \\n        if done:\\n            break # Skips adding last observation to q-table\\n\\n        for i in range(len(obs)):\\n            step = q_table_params[\"step\"][i]\\n            \\n            power = 10 ** step\\n            \\n            if i == 2:\\n                val = math.degrees(obs[i])\\n            else:\\n                val = obs[i]\\n            \\n            index = int(round(val, step) * power)\\n            q_table[i][index] += 1\\n\\nprint(q_table[0])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Check to make sure that all continuous observations have translated to discrete spaces \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "episodes = 10000\n",
    "for i_ep in range(episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            break # Skips adding last observation to q-table\n",
    "\n",
    "        for i in range(len(obs)):\n",
    "            step = q_table_params[\"step\"][i]\n",
    "            \n",
    "            power = 10 ** step\n",
    "            \n",
    "            if i == 2:\n",
    "                val = math.degrees(obs[i])\n",
    "            else:\n",
    "                val = obs[i]\n",
    "            \n",
    "            index = int(round(val, step) * power)\n",
    "            q_table[i][index] += 1\n",
    "\n",
    "print(q_table[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.5 # Learning rate -> how fast the values are propagated throughout the q-table\n",
    "GAMMA = 0.9 # Discount factor -> how much the rewards are discounted from future steps\n",
    "EPSILON = 0.1 # Greedy function -> exploration vs exploitation -> chance for the agent to make an educated guess\n",
    "\n",
    "REWARD_SHAPING = -1000\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Generating Docstrings](https://queirozf.com/entries/python-docstrings-reference-examples#:~:text=Python%20Docstrings%3A%20Reference%20%26%20Examples%201%20ReStructuredText%20%28reST%29,description%20of%20function.%20...%204%20Doctest%20Permalink.%20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete(radians):\n",
    "    return math.degrees(radians)\n",
    "\n",
    "def greedy():\n",
    "    return np.random.rand() > EPSILON\n",
    "\n",
    "def q_func(reward, q_current_value, q_forward_value):\n",
    "    \"\"\"Returns q_value to update q_table with\"\"\"\n",
    "    \n",
    "    q_value = ALPHA * (reward + GAMMA * q_forward_value - q_current_value)\n",
    "    return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self):\n",
    "        self.q_table = np.zeros((29, 2))\n",
    "        \n",
    "        self.env = None\n",
    "        self.make()\n",
    "\n",
    "    def _forward(self, observation):\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, render=False):\n",
    "        env = self.env \n",
    "        \n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        for i in range(200): \n",
    "            if render: \n",
    "                env.render()\n",
    "                \n",
    "            action = self._forward(obs)\n",
    "            \n",
    "            obs, rew, done, _ = env.step(action) \n",
    "\n",
    "        print(done)\n",
    "            \n",
    "    \n",
    "    def make(self):\n",
    "        if self.env is None: \n",
    "            self.env = gym.make(ENV_NAME)\n",
    "    \n",
    "    def close(self):\n",
    "        if self.env is not None: \n",
    "            self.env.close()\n",
    "            self.env = None\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "None (<class 'NoneType'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jason\\Desktop\\2022-05-23-PRG - RPI Learning and Advanced Game AI Summer 2022\\week2\\class0\\cartpole-ql.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week2/class0/cartpole-ql.ipynb#ch0000015?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m QAgent()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week2/class0/cartpole-ql.ipynb#ch0000015?line=1'>2</a>\u001b[0m a\u001b[39m.\u001b[39;49mevaluate()\n",
      "\u001b[1;32mc:\\Users\\jason\\Desktop\\2022-05-23-PRG - RPI Learning and Advanced Game AI Summer 2022\\week2\\class0\\cartpole-ql.ipynb Cell 12'\u001b[0m in \u001b[0;36mQAgent.evaluate\u001b[1;34m(self, render)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week2/class0/cartpole-ql.ipynb#ch0000011?line=18'>19</a>\u001b[0m         env\u001b[39m.\u001b[39mrender()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week2/class0/cartpole-ql.ipynb#ch0000011?line=20'>21</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(obs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week2/class0/cartpole-ql.ipynb#ch0000011?line=22'>23</a>\u001b[0m     obs, rew, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/2022-05-23-PRG%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week2/class0/cartpole-ql.ipynb#ch0000011?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(done)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py:49\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/time_limit.py?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/time_limit.py?line=38'>39</a>\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/time_limit.py?line=39'>40</a>\u001b[0m \n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/time_limit.py?line=40'>41</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/time_limit.py?line=46'>47</a>\u001b[0m \u001b[39m        \"TimeLimit.truncated\"=False if the environment terminated\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/time_limit.py?line=47'>48</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/time_limit.py?line=48'>49</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/time_limit.py?line=49'>50</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/time_limit.py?line=50'>51</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=34'>35</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=35'>36</a>\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=36'>37</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:123\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/cartpole.py?line=120'>121</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/cartpole.py?line=121'>122</a>\u001b[0m     err_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m!r}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(action)\u001b[39m}\u001b[39;00m\u001b[39m) invalid\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/cartpole.py?line=122'>123</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(action), err_msg\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/cartpole.py?line=123'>124</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCall reset before using step method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/cartpole.py?line=124'>125</a>\u001b[0m     x, x_dot, theta, theta_dot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "\u001b[1;31mAssertionError\u001b[0m: None (<class 'NoneType'>) invalid"
     ]
    }
   ],
   "source": [
    "a = QAgent()\n",
    "a.evaluate()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7c1fa83a45090c7dcf13129008a160cc437ddcdb7cd57cdda701723205ddb7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
