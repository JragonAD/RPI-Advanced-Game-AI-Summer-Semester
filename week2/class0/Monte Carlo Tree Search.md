# Monte Carlo Tree Search

## Bandit Problems

- At each step, pull one arm
- Noisy/random reward signal in the range [0, 1]
- Different average reward
- **Task:** maximize reward (Minimize regret)

## Which arm to pull?

- Pull all arms equally often?
- Only pull the arm that has given the best results so far?
- Mostly pull the "best" arm, but sometimes pull one of the others?
- An example of exploration/exploitation dilemma
- Principled solution?

