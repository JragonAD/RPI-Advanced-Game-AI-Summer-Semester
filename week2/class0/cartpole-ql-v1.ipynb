{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Using Q-Learning\n",
    "\n",
    "[CartPole Documentation](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n",
    "\n",
    "| Num | Action                 |\n",
    "|-----|------------------------|\n",
    "| 0   | Push cart to the left  |\n",
    "| 1   | Push cart to the right |\n",
    "\n",
    "**Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "\n",
    "| Num | Observation           | Min                 | Max               |\n",
    "|-----|-----------------------|---------------------|-------------------|\n",
    "| 0   | Cart Position         | -4.8                | 4.8               |\n",
    "| 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "| 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "\n",
    "**Note:** While the ranges above denote the possible values for observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "\n",
    "- The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates if the cart leaves the `(-2.4, 2.4)` range.\n",
    "- The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Table\n",
    "\n",
    "| Num | Start | Step Width | Steps (Inc. 0) |\n",
    "|-----|-------|------------|----------------|\n",
    "| 0   | -2.4  | 0.1        | 48 + 1         |\n",
    "| 1   | -4    | 0.1       | 80 + 1        |\n",
    "| 2   | -12   | 1°         | 24 + 1         |\n",
    "| 3   | -4    | 0.1       | 80 + 1        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(epsilon):\n",
    "    \n",
    "    return np.random.rand() > epsilon\n",
    "\n",
    "\n",
    "def q_func(reward, q_current_value, q_forward_value):\n",
    "    \"\"\"Returns q_value to update q_table with\"\"\"\n",
    "\n",
    "    q_value = q_current_value + ALPHA * (reward + GAMMA * q_forward_value - q_current_value)\n",
    "    return q_value\n",
    "\n",
    "\n",
    "def display(values):\n",
    "    plt.plot(values)\n",
    "\n",
    "    plt.title(\"Training Data\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1000\n",
    "ALPHA = 0.5\n",
    "GAMMA = 0.9\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.999\n",
    "EPSILON_MIN = 0.0001\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, type=\"QLEARNING\", env_name=ENV_NAME):\n",
    "        env = gym.make(env_name)\n",
    "        self.env = env\n",
    "        \n",
    "        self.type = type\n",
    "\n",
    "        self.rewards = []\n",
    "        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.epsilon = EPSILON\n",
    "\n",
    "    def evaluate(self, episodes=EPISODES):\n",
    "        rewards = self.rewards\n",
    "        q_table = self.q_table\n",
    "        epsilon = self.epsilon\n",
    "        \n",
    "        env = self.env\n",
    "        type = self.type\n",
    "\n",
    "        for _ in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                if not greedy(epsilon):\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(q_table[state])\n",
    "\n",
    "                forward_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                q_current = q_table[state, action]\n",
    "                if type == \"QLEARNING\":\n",
    "                    q_forward = np.max(q_table[forward_state])\n",
    "                else:\n",
    "                    q_forward = np.average(q_table[forward_state])\n",
    "\n",
    "                q_table[state, action] = q_func(reward, q_current, q_forward)\n",
    "\n",
    "                state = forward_state\n",
    "\n",
    "            rewards.append(reward)\n",
    "            epsilon *= EPSILON_DECAY\n",
    "            epsilon = max(EPSILON_MIN, epsilon)\n",
    "\n",
    "        self.rewards = rewards\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = q_table\n",
    "        \n",
    "        return rewards\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7c1fa83a45090c7dcf13129008a160cc437ddcdb7cd57cdda701723205ddb7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
