{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car QL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://ha-nguyen-39691.medium.com/playing-mountain-car-with-q-learning-and-sarsa-4e7327f9e35c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPS = 100000\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.9\n",
    "EPSILON = 0.2\n",
    "INTERVAL = 1000\n",
    "STEPS = 30\n",
    "\n",
    "Q_LEARNING = \"QLEARNING\"\n",
    "SARSA = \"SARSA\"\n",
    "\n",
    "ENV_NAME = \"MountainCar-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(\n",
    "        self,\n",
    "        steps=STEPS,\n",
    "        agent_type=Q_LEARNING,\n",
    "        alpha=ALPHA,\n",
    "        gamma=GAMMA,\n",
    "        env_name=ENV_NAME,\n",
    "    ):\n",
    "        shape = (steps, steps)\n",
    "\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "        action_space_n = env.action_space.n\n",
    "        shape += action_space_n\n",
    "\n",
    "        self.q_table = np.zeros(shape)\n",
    "\n",
    "        self.env_low = env.observation_space.low\n",
    "        self.env_high = env.observation_space.high\n",
    "        self.discretized_env = (self.env_high - self.env_low) / steps\n",
    "\n",
    "        self.steps = steps\n",
    "        self.agent_type = agent_type\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _discretize(self, state):\n",
    "        discretized_state = (state - self.env_low) / self.discretized_env\n",
    "        discretized_state.astype(int)\n",
    "\n",
    "        return discretized_state\n",
    "\n",
    "    def _q_func(self, reward, q_current_value, q_forward_value):\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "\n",
    "        q_value = q_current_value + alpha * (\n",
    "            reward + gamma * q_forward_value - q_current_value\n",
    "        )\n",
    "        return q_value\n",
    "\n",
    "    def _get_sub_table(self, state):\n",
    "        q_sub_table = self.q_table\n",
    "\n",
    "        discrete_state = self._discretize(state)\n",
    "\n",
    "        for index in discrete_state:\n",
    "            q_sub_table = q_sub_table[index]\n",
    "\n",
    "        return q_sub_table\n",
    "\n",
    "    def get_value(self, state, action=None):\n",
    "        q_value = self._get_sub_table(state)\n",
    "\n",
    "        if action is not None:\n",
    "            q_value = q_value[action]\n",
    "\n",
    "        return q_value\n",
    "\n",
    "    def _update_value(self, state, action, update_value):\n",
    "        q_sub_table = self._get_sub_table(state)\n",
    "\n",
    "        q_sub_table[action] = update_value\n",
    "\n",
    "    def update_table(self, state, forward_state, action, reward):\n",
    "        agent_type = self.agent_type\n",
    "\n",
    "        q_current = self.get_value(state, action)\n",
    "        q_forward = self.get_value(forward_state)\n",
    "        \n",
    "        if agent_type == Q_LEARNING:\n",
    "            q_forward = np.max(q_forward)\n",
    "        else:\n",
    "            q_forward = np.average(q_forward)\n",
    "\n",
    "        update_value = self._q_func(reward, q_current, q_forward)\n",
    "        self._update_value(state, action, update_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# space discretization\n",
    "def getState(state, env_low = env_low, env_high = env_high, bins = bins):\n",
    "    \"\"\"Returns the discretized position and velocity of an observation\"\"\"\n",
    "    discretized_env = (env_high - env_low) / bins\n",
    "    discretized_pos = int((state[0] - env_low[0]) / discretized_env[0])\n",
    "    discretized_vel = int((state[1] - env_low[1]) / discretized_env[1])\n",
    "    return discretized_pos, discretized_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# choose action\n",
    "def chooseAction(pos, vel, q_table, epsilon = epsilon):\n",
    "    \"\"\"Choose action based on an epsilon greedy strategy\"\"\"\n",
    "    if random.random() < epsilon: # explore\n",
    "        action = env.action_space.sample()\n",
    "    else: # exploit\n",
    "        action = np.argmax(q_table[pos][vel])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game no.:  0 epsilon:  0.19999600003999962 with reward:  -200.0\n",
      "Game no.:  10000 epsilon:  0.16374287572423812 with reward:  -154.0\n",
      "Game no.:  20000 epsilon:  0.1340613279519637 with reward:  -200.0\n",
      "Game no.:  30000 epsilon:  0.10976013199201175 with reward:  -148.0\n",
      "Game no.:  40000 epsilon:  0.08986399552315864 with reward:  -169.0\n",
      "Game no.:  50000 epsilon:  0.07357441672877993 with reward:  -168.0\n",
      "Game no.:  60000 epsilon:  0.060237637615224854 with reward:  -176.0\n",
      "Game no.:  70000 epsilon:  0.04931840640802185 with reward:  -146.0\n",
      "Game no.:  80000 epsilon:  0.04037849601877588 with reward:  -149.0\n",
      "Game no.:  90000 epsilon:  0.033059116453387716 with reward:  -166.0\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jason\\Desktop\\PRG-2022-05-23 - RPI Learning and Advanced Game AI Summer 2022\\week2\\class0\\mountaincar-ql-v0.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/PRG-2022-05-23%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week2/class0/mountaincar-ql-v0.ipynb#ch0000007?line=10'>11</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/PRG-2022-05-23%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week2/class0/mountaincar-ql-v0.ipynb#ch0000007?line=11'>12</a>\u001b[0m     \u001b[39m# render for the last 10 episodes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/PRG-2022-05-23%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week2/class0/mountaincar-ql-v0.ipynb#ch0000007?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m ep \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (n_eps \u001b[39m-\u001b[39m \u001b[39m10\u001b[39m): \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/PRG-2022-05-23%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week2/class0/mountaincar-ql-v0.ipynb#ch0000007?line=13'>14</a>\u001b[0m         env\u001b[39m.\u001b[39;49mrender()\u001b[39m# next action\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/PRG-2022-05-23%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week2/class0/mountaincar-ql-v0.ipynb#ch0000007?line=14'>15</a>\u001b[0m     action \u001b[39m=\u001b[39m chooseAction(pos, vel, q_table_q)\u001b[39m# next state\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/PRG-2022-05-23%20-%20RPI%20Learning%20and%20Advanced%20Game%20AI%20Summer%202022/week2/class0/mountaincar-ql-v0.ipynb#ch0000007?line=15'>16</a>\u001b[0m     next_state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gym\\core.py:328\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/core.py?line=325'>326</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/core.py?line=326'>327</a>\u001b[0m     \u001b[39m\"\"\"Renders the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/core.py?line=327'>328</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=45'>46</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=46'>47</a>\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=47'>48</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=48'>49</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=49'>50</a>\u001b[0m     )\n\u001b[1;32m---> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/wrappers/order_enforcing.py?line=50'>51</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py:237\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/mountain_car.py?line=229'>230</a>\u001b[0m gfxdraw\u001b[39m.\u001b[39mfilled_polygon(\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/mountain_car.py?line=230'>231</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf,\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/mountain_car.py?line=231'>232</a>\u001b[0m     [(flagx, flagy2), (flagx, flagy2 \u001b[39m-\u001b[39m \u001b[39m10\u001b[39m), (flagx \u001b[39m+\u001b[39m \u001b[39m25\u001b[39m, flagy2 \u001b[39m-\u001b[39m \u001b[39m5\u001b[39m)],\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/mountain_car.py?line=232'>233</a>\u001b[0m     (\u001b[39m204\u001b[39m, \u001b[39m204\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/mountain_car.py?line=233'>234</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/mountain_car.py?line=235'>236</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mflip(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf, \u001b[39mFalse\u001b[39;00m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/mountain_car.py?line=236'>237</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscreen\u001b[39m.\u001b[39;49mblit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msurf, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m))\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/mountain_car.py?line=237'>238</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/jason/AppData/Local/Continuum/Anaconda3/lib/site-packages/gym/envs/classic_control/mountain_car.py?line=238'>239</a>\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "q_table_q = QTable()\n",
    "rewards_q = []  # training\n",
    "\n",
    "for ep in range(N_EPS):\n",
    "    state = env.reset()\n",
    "    current_reward = 0\n",
    "    done = False  # discretize the state\n",
    "\n",
    "    while not done:\n",
    "        # render for the last 10 episodes\n",
    "        if ep >= (N_EPS - 10):\n",
    "            env.render()  # next action\n",
    "        action = np.argmax(q_table_q.get_value(state))\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # discretize next state\n",
    "        next_pos, next_vel = getState(next_state)\n",
    "        if done and next_state[0] >= env.goal_position:\n",
    "            q_table_q._update_value(next_state, action, reward)\n",
    "\n",
    "        else:\n",
    "            # update Q value: Q(S, A) <-- Q(S, A) + alpha [R + gamma * Q(S', A') - Q(S, A)]\n",
    "            q_table_q.update_table(state, next_state, action, reward)\n",
    "\n",
    "        # reassign state, action, reward\n",
    "        state = next_state\n",
    "        current_reward += reward  # update EPSILON\n",
    "    if EPSILON > 0:\n",
    "        EPSILON *= (N_EPS - 2) / N_EPS\n",
    "\n",
    "    # periodically print out result\n",
    "    if ep % INTERVAL == 0:\n",
    "        print(\"Game no.: \", ep, \"EPSILON: \", EPSILON, \"with reward: \", current_reward)\n",
    "    rewards_q.append(current_reward)\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Professor recommended to move on to Double DQN as continuous MountainCar is difficult to discretize and requires a lot of training"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7c1fa83a45090c7dcf13129008a160cc437ddcdb7cd57cdda701723205ddb7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
